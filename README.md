**Final Paper: Unsupervised Tree Species Segmentation from Drone Imagery**

### Overview
This project implements the V1 pipeline for unsupervised tree species segmentation using high-resolution RGB drone imagery, as part of a research effort to differentiate tree species without labeled data. The pipeline leverages DINOv2 for deep feature extraction, HR-Dv2 for upsampling, and K-Means clustering to segment tree crowns. The primary research question is: How can unsupervised ML methods, using deep visual feature extraction and clustering, accurately segment and differentiate tree species in high-resolution RGB drone imagery without labeled training data?

The project is developed in a WSL2 Ubuntu environment using Nushell, with Python 3.10.17 managed by `uv`. It utilizes an NVIDIA RTX 3050 for GPU acceleration and includes tools like `black` (code formatting) and `ruff` (linting).

### Project Structure
- `main.py`: Main script orchestrating the V1 pipeline (feature extraction, clustering, output).
- `upsampler.py`: Implements HighResDV2 for upsampled DINOv2 feature extraction (copied from HR-Dv2ÔÇÖs `high_res.py`).
- `patch.py`, `transform.py`: Supporting modules from HR-Dv2 for DINOv2 modifications and transformations.
- `pyproject.toml`: Project metadata and `ruff` configuration.
- `README.md`: This file.
- `uv.lock`: Dependency lockfile generated by `uv`.

### Prerequisites
- **System**: WSL2 with Ubuntu 24.04 on Windows 11.
- **Hardware**: NVIDIA RTX 3050 (6GB VRAM) for CUDA acceleration.
- **Software**:
  - Nushell (for shell commands).
  - Git (for cloning repositories).
  - NVIDIA drivers and CUDA toolkit (for WSL2 GPU support).
  - **Rust (if using ty)**: Install via `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh` for compiling ty.

### Setup Instructions
1. **Install WSL2 and Ubuntu**
   - In PowerShell (as Administrator):
     ```powershell
     wsl --install
     ```
   - Install Ubuntu from the Microsoft Store if needed, then update:
     ```bash
     sudo apt update && sudo apt upgrade -y
     sudo apt install -y build-essential curl libssl-dev zlib1g-dev libbz2-dev \
     libreadline-dev libsqlite3-dev libncursesw5-dev xz-utils libffi-dev liblzma-dev
     ```

2. **Install `uv`**
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   uv --version
   ```

3. **Clone and Set Up the Project**
   ```bash
   git clone <your-repo-url> final-paper
   cd final-paper
   uv venv --python ~/.cache/uv/python/cpython-3.10.17-linux-x86_64-gnu/bin/python
   overlay use .venv/bin/activate.nu
   ```

4. **Install Dependencies**
   ```bash
   uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
   uv pip install timm scikit-learn numpy opencv-python
   uv add --dev black ruff git+https://github.com/astral-sh/ty
   uv sync
   uv lock
   ```

5. **Copy HR-Dv2 Files**
   - Clone HR-Dv2 outside `final-paper` and copy necessary files:
     ```bash
     cd ~
     git clone https://github.com/tldr-group/HR-Dv2
     cd ~/final-paper
     cp ../HR-Dv2/hr_dv2/high_res.py ./upsampler.py
     cp ../HR-Dv2/hr_dv2/patch.py ./patch.py
     cp ../HR-Dv2/hr_dv2/transform.py ./transform.py
     ```

6. **Configure `ruff`**
   - Ensure `pyproject.toml` includes:
     ```toml
     [tool.ruff]
     line-length = 88
     exclude = [".venv", ".git"]
     ```

### Usage
1. **Prepare an Image**
   - Download a sample RGB drone image from NEON AOP (https://data.neonscience.org/data-products/DP3.30010.001).
   - Place it in `final-paper` (e.g., `drone_image.jpg`).

2. **Update `main.py`**
   - Edit `main.py` to set the image path:
     ```python
     image_path = "drone_image.jpg"
     ```

3. **Run the Pipeline**
   ```bash
   python main.py
   ```
   - Outputs `segmented_image.jpg` with clustered tree segments.

4. **Format, Lint, and Type Check**
   ```bash
   black main.py
   ruff check main.py --fix
   ty check main.py  # Performs static type checking
   ```

### Technical Details
**Pipeline:**
- **Feature Extraction**: Uses HighResDV2 from `upsampler.py` to extract upsampled DINOv2 features (`dinov2_vits14`, stride=4, float16 for RTX 3050).
- **Clustering**: Applies K-Means (`n_clusters=5`) to flattened features.
- **Output**: Resizes cluster labels to original image size and saves as `segmented_image.jpg`.

**Optimization:**
- `float16` dtype reduces VRAM usage on RTX 3050 (4GB).
- `stride=4` balances resolution and memory.

**Dependencies:**
- `torch`, `torchvision`: GPU-accelerated tensor operations.
- `timm`: DINOv2 model loading.
- `scikit-learn`: K-Means clustering.
- `numpy`, `opencv-python`: Image processing.
- `black`, `ruff`: Code quality tools.
- **ty**: Static type checker for Python, developed by Astral, for fast type analysis (pre-alpha, focuses on type annotations).
- `uv`: Dependency management and cross-platform lockfile generation.

### Troubleshooting
- **CUDA Errors:**
  ```bash
  nvidia-smi
  python -c "import torch; print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"
  ```
  - Ensure WSL2 GPU support (https://docs.microsoft.com/en-us/windows/wsl/tutorials/gpu-compute).
- **VRAM Issues**:
  - Edit `main.py` to use `stride=8` or `dinov2_vits14` with a smaller model.
- **Dependency Issues**:
  ```bash
  uv pip install <package>
  uv lock
  ```

### Next Steps
- Test with NEON AOP imagery.
- Experiment with transforms in `transform.py` for improved features.
- Evaluate segmentation with ground-truth subset (Pixel Accuracy, mIoU).
- Extend to V2 (U2Seg) or V3 (DynaSeg) pipelines.

### References
- Docherty et al. (2024). Upsampling DINOv2 features. https://doi.org/10.48550/ARXIV.2410.19836
- HR-Dv2: https://github.com/tldr-group/HR-Dv2
- NEON AOP: https://data.neonscience.org/data-products/DP3.30010.001
- DINOv2: https://github.com/facebookresearch/dinov2

### License
MIT
