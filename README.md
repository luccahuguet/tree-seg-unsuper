# Tree Segmentation with DINOv2

Modern unsupervised tree segmentation using DINOv2 Vision Transformers and K-means clustering for aerial drone imagery.

## ğŸš€ What's New - Modern Architecture

**Clean API with professional patterns:**
- ğŸ—ï¸ **Dataclass Configuration** - Type-safe, validated config objects
- ğŸ“¦ **Result Objects** - Structured returns instead of tuples  
- ğŸ¯ **OutputManager** - Intelligent file naming and management
- ğŸ§¹ **Clean Interface** - Simple API for both quick and advanced usage
- âœ… **Pure Modern Code** - No legacy cruft

## ğŸŒ³ Project Overview

This project implements an unsupervised tree segmentation pipeline that:
- Uses DINOv2 Vision Transformers to extract deep features from aerial images
- Applies K-means clustering for segmentation
- Supports v1.5 (patch + attention features) with automatic K-selection
- Generates high-quality visualization outputs with config-based naming

## ğŸ“ Project Structure

```
final-paper/
â”œâ”€â”€ src/                       # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ kmeans_segmentation.py
â”‚   â”œâ”€â”€ patch.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ upsampler.py
â”‚
â”œâ”€â”€ notebooks/                 # Cloud-ready notebooks
â”‚   â”œâ”€â”€ tree_seg_kaggle.ipynb  # Kaggle notebook
â”‚   â””â”€â”€ tree_seg_colab.ipynb   # Google Colab notebook
â”‚
â”œâ”€â”€ deployment/                # Cloud deployment package
â”‚   â”œâ”€â”€ tree_seg_deployment.zip
â”‚   â”œâ”€â”€ run.sh                 # Cloud execution script
â”‚   â”œâ”€â”€ requirements.txt       # Dependencies
â”‚   â”œâ”€â”€ tree_seg_local.py      # Main script
â”‚   â”œâ”€â”€ config.yaml           # Configuration
â”‚   â”œâ”€â”€ README.md             # Deployment instructions
â”‚   â””â”€â”€ src/                  # Source code copy
â”‚
â”œâ”€â”€ input/                     # Input images (local runs)
â”œâ”€â”€ output/                    # Output results
â”œâ”€â”€ tree_seg_local.py          # Main script for local execution
â”œâ”€â”€ deploy_to_cloud.py         # Deployment automation
â”œâ”€â”€ config.yaml               # Configuration
â”œâ”€â”€ pyproject.toml            # Project metadata
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### API Usage

**Simple one-liner:**
```python
from tree_seg import segment_trees

# Process single image with smart defaults
results = segment_trees(
    "/kaggle/input/drone-imagery/image.jpg",
    model="base",
    auto_k=True
)
```

**Advanced usage with full control:**
```python
from tree_seg import TreeSegmentation, Config

# Create configuration
config = Config(
    model_name="base",      # small, base, large, giant
    auto_k=True,           # Automatic K selection
    elbow_threshold=0.1,   # Sensitive elbow detection
    use_hatching=True,     # Visual distinction
    edge_width=2
)

# Process with modern API
segmenter = TreeSegmentation(config)
results, paths = segmenter.process_and_visualize("image.jpg")

print(f"Used K = {results.n_clusters_used}")
print(f"Files: {paths.all_paths()}")
```


### Cloud GPU Execution

#### Option 1: Kaggle
1. Go to [Kaggle Notebooks](https://www.kaggle.com/code)
2. Upload `notebooks/tree_seg_kaggle.ipynb`
3. Attach your dataset or upload images
4. Run on Kaggle GPU

#### Option 2: Google Colab
1. Go to [Google Colab](https://colab.research.google.com/)
2. Upload `notebooks/tree_seg_colab.ipynb`
3. Upload your images when prompted
4. Run on Colab GPU

#### Option 3: Any Cloud GPU VM
1. Upload `deployment/tree_seg_deployment.zip` to your VM
2. Unzip and run:
   ```bash
   unzip tree_seg_deployment.zip
   chmod +x run.sh
   ./run.sh
   ```

## âš™ï¸ Configuration

### Configuration

```python
from tree_seg import Config

config = Config(
    # Input/Output
    input_dir="/kaggle/input/drone-imagery",
    output_dir="/kaggle/working/output", 
    filename=None,              # Process all images in directory
    
    # Model settings
    model_name="base",          # small/base/large/giant
    version="v1.5",             # Current version
    stride=4,                   # Balance of speed vs quality
    
    # Clustering (automatic K-selection recommended)
    auto_k=True,                # Let elbow method choose K
    elbow_threshold=0.1,        # Sensitivity (0.05-0.3)
    k_range=(3, 10),            # K search range
    n_clusters=6,               # Only used when auto_k=False
    
    # Visualization
    overlay_ratio=4,            # Transparency (1=opaque, 10=transparent)
    edge_width=2,               # Border thickness
    use_hatching=True,          # Pattern distinction
    use_pca=False               # Keep full feature space
)

# Automatic validation
config.validate()  # Raises ValueError if invalid
```

### Configuration Tips

**Elbow Threshold Values:**
- `0.05-0.1`: Sensitive (finds more clusters)
- `0.1-0.2`: Balanced (recommended)  
- `0.2-0.3`: Conservative (fewer clusters)

**Model Sizes:**
- `small`: Fast, good for testing
- `base`: Best balance (recommended)
- `large`: Higher quality, slower
- `giant`: Maximum quality, very slow

## ğŸ“Š Output Files

### Smart Filename Generation

Files now use **config-based naming** for easy identification:

```
{hash}_{version}_{model}_{stride}_{clustering}_type.png
```

**Examples:**
- `a3f7_v1-5_base_str4_et0-1_segmentation_legend.png`
- `a3f7_v1-5_base_str4_et0-1_edge_overlay.png`
- `a3f7_v1-5_base_str4_et0-1_side_by_side.png`
- `a3f7_v1-5_base_str4_et0-1_elbow_analysis.png`

**Filename Components:**
- `a3f7`: 4-char hash of original filename (prevents collisions)
- `v1-5`: Version used
- `base`: Model size
- `str4`: Stride value
- `et0-1`: Elbow threshold (or `k6` for fixed K)

**File Types:**
- `segmentation_legend.png`: Colored map with cluster legend
- `edge_overlay.png`: Original image with colored boundaries
- `side_by_side.png`: Original vs segmentation comparison
- `elbow_analysis.png`: K-selection analysis (when auto_k=True)

## ğŸ”§ Dependencies

- **PyTorch** >= 2.0.0
- **torchvision** >= 0.15.0
- **timm** >= 0.9.0 (for DINOv2 models)
- **opencv-python** >= 4.8.0
- **matplotlib** >= 3.7.0
- **scikit-learn** >= 1.3.0
- **Pillow** >= 10.0.0
- **numpy** >= 1.24.0
- **xformers** >= 0.0.20 (optional, for memory efficiency)

## ğŸ¯ Model Variants

- **v1**: Uses only patch features from DINOv2
- **v1.5**: Uses both patch features and attention features (recommended)

## ğŸ–¼ï¸ Supported Image Formats

- JPEG (.jpg, .jpeg)
- PNG (.png)
- TIFF (.tif, .tiff)

## ğŸš€ Deployment Automation

Use the deployment script to generate cloud-ready files:

```bash
python deploy_to_cloud.py
```

This creates:
- Kaggle notebook in `notebooks/`
- Colab notebook in `notebooks/`
- Deployment package in `deployment/`

## ğŸ“ Usage Examples

### Process a single image:
```python
from tree_seg_local import tree_seg

tree_seg(
    input_dir="input",
    output_dir="output", 
    filename="drone_image.jpg",
    n_clusters=6,
    version="v1.5"
)
```

### Process all images in directory:
```python
tree_seg(
    input_dir="input",
    output_dir="output",
    filename=None,  # Process all images
    n_clusters=8,
    stride=2  # Higher resolution
)
```

## ğŸ” Troubleshooting

### Common Issues:

1. **CUDA out of memory**: Reduce `stride` or use smaller images
2. **Import errors**: Ensure all dependencies are installed
3. **No GPU detected**: Script will automatically use CPU (slower)

### Performance Tips:

- Use `stride=2` for higher resolution (slower)
- Use `stride=8` for faster processing (lower resolution)
- Adjust `n_clusters` based on image complexity
- Use `version="v1.5"` for better results

## ğŸ“„ License

This project is part of a research paper on unsupervised tree segmentation.

## ğŸ¤ Contributing

This is a research project. For questions or issues, please refer to the paper or contact the authors.

## ğŸ—ºï¸ Roadmap

- **V1:** Patch features only, K-Means clustering.
- **V1.5 (Current):** Patch + attention features, K-Means clustering, PCA cluster visualization.
- **V2:** U2Seg (advanced unsupervised segmentation).
- **V3:** DynaSeg (dynamic fusion segmentation).
- **V4:** Multispectral extension.

## ğŸ”œ Next Steps

- Test with diverse NEON AOP imagery for robustness.
- Explore `transform.py` for advanced feature augmentation (e.g., rotations, flips).
- Evaluate segmentation quality using ground-truth data (metrics: Pixel Accuracy, mIoU).
- Extend to V2 (U2Seg) or V3 (DynaSeg) pipelines for improved segmentation.

## ğŸ“š References

- Docherty et al. (2024). Upsampling DINOv2 features. https://doi.org/10.48550/ARXIV.2410.19836
- HR-Dv2: https://github.com/tldr-group/HR-Dv2
- NEON AOP: https://data.neonscience.org/data-products/DP3.30010.001
- DINOv2: https://github.com/facebookresearch/dinov2

