{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "be0c56c5-8daf-49b8-ab35-aaaee01df13c",
                "_uuid": "c459e38b-2f6e-423f-889b-65728ad7a7ea",
                "execution": {
                    "iopub.execute_input": "2025-06-11T13:54:41.942799Z",
                    "iopub.status.busy": "2025-06-11T13:54:41.942608Z",
                    "iopub.status.idle": "2025-06-11T13:54:42.616402Z",
                    "shell.execute_reply": "2025-06-11T13:54:42.615481Z",
                    "shell.execute_reply.started": "2025-06-11T13:54:41.942783Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Change to a safe directory first\n",
                "%cd /kaggle/working\n",
                "\n",
                "# Remove the old project directory if it exists\n",
                "!rm -rf /kaggle/working/project\n",
                "\n",
                "# Clone the repository again\n",
                "!git clone https://github.com/luccahuguet/tree-seg-unsuper /kaggle/working/project\n",
                "\n",
                "# Change into the new project directory\n",
                "%cd /kaggle/working/project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "1b1c11af-473a-43d2-88a9-5c695f8239ef",
                "_uuid": "c426ed0e-5862-4dbb-9ac6-f59c8d442399",
                "execution": {
                    "iopub.execute_input": "2025-06-11T13:54:42.617973Z",
                    "iopub.status.busy": "2025-06-11T13:54:42.617621Z",
                    "iopub.status.idle": "2025-06-11T13:54:53.608194Z",
                    "shell.execute_reply": "2025-06-11T13:54:53.607167Z",
                    "shell.execute_reply.started": "2025-06-11T13:54:42.617932Z"
                },
                "jupyter": {
                    "source_hidden": true
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Uninstall conflicting packages\n",
                "%pip uninstall -y torchaudio fastai\n",
                "\n",
                "# Install dependencies with CUDA 12.4\n",
                "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
                "%pip install timm\n",
                "%pip install xformers --index-url https://download.pytorch.org/whl/cu124"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "d00e6f26-0744-4b6c-82da-d3416c81a800",
                "_uuid": "e9b9d85b-edb8-4e80-87cc-e550c39d782f",
                "execution": {
                    "iopub.execute_input": "2025-06-11T13:54:53.609808Z",
                    "iopub.status.busy": "2025-06-11T13:54:53.609504Z",
                    "iopub.status.idle": "2025-06-11T13:54:53.616379Z",
                    "shell.execute_reply": "2025-06-11T13:54:53.615754Z",
                    "shell.execute_reply.started": "2025-06-11T13:54:53.609779Z"
                },
                "jupyter": {
                    "source_hidden": true
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# gpu stuff\n",
                "import os\n",
                "import torch\n",
                "\n",
                "def print_gpu_info():\n",
                "    if torch.cuda.is_available():\n",
                "        gpu_idx = torch.cuda.current_device()\n",
                "        gpu_name = torch.cuda.get_device_name(gpu_idx)\n",
                "        total_mem = torch.cuda.get_device_properties(gpu_idx).total_memory / (1024**3)\n",
                "        print(f\"GPU: {gpu_name}\")\n",
                "        print(f\"Total VRAM: {total_mem:.2f} GB\")\n",
                "    else:\n",
                "        print(\"No CUDA-compatible GPU found.\")\n",
                "\n",
                "def setup_segmentation(output_dir):\n",
                "    print_gpu_info()\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    print(f\"Using device: {device}\")\n",
                "    return device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "6bd6cebd-f68e-4155-a24b-6cedfb51eb1c",
                "_uuid": "86b1668b-5300-4d85-a019-dad1fe1bfff4",
                "execution": {
                    "iopub.execute_input": "2025-06-11T14:14:56.089895Z",
                    "iopub.status.busy": "2025-06-11T14:14:56.089363Z",
                    "iopub.status.idle": "2025-06-11T14:14:56.094808Z",
                    "shell.execute_reply": "2025-06-11T14:14:56.094101Z",
                    "shell.execute_reply.started": "2025-06-11T14:14:56.089870Z"
                },
                "jupyter": {
                    "source_hidden": true
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# init model\n",
                "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
                "from src.upsampler import HighResDV2\n",
                "\n",
                "def init_model_and_preprocess(model_name, stride, device):\n",
                "    model = HighResDV2(model_name, stride=stride, dtype=torch.float16).to(device)\n",
                "    model.eval()\n",
                "    model.set_transforms([lambda x: x], [lambda x: x])  # Identity transforms\n",
                "    preprocess = Compose(\n",
                "        [\n",
                "            Resize((518, 518)),\n",
                "            ToTensor(),\n",
                "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ]\n",
                "    )\n",
                "    return model, preprocess"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "f09ae3e5-25d6-4eb2-980b-134100e0fded",
                "_uuid": "fc016496-f54c-4207-ad48-aeae7ca59822",
                "collapsed": false,
                "execution": {
                    "iopub.execute_input": "2025-06-11T14:15:00.927567Z",
                    "iopub.status.busy": "2025-06-11T14:15:00.926968Z",
                    "iopub.status.idle": "2025-06-11T14:15:00.937330Z",
                    "shell.execute_reply": "2025-06-11T14:15:00.936598Z",
                    "shell.execute_reply.started": "2025-06-11T14:15:00.927532Z"
                },
                "jupyter": {
                    "outputs_hidden": false
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Setup cell\n",
                "import os\n",
                "import torch\n",
                "import numpy as np\n",
                "from sklearn.cluster import KMeans\n",
                "from PIL import Image\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib import colors\n",
                "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
                "from src.upsampler import HighResDV2\n",
                "from src.transform import get_shift_transforms, get_flip_transforms, get_rotation_transforms, combine_transforms, iden_partial\n",
                "import traceback\n",
                "\n",
                "def print_gpu_info():\n",
                "    if torch.cuda.is_available():\n",
                "        gpu_idx = torch.cuda.current_device()\n",
                "        gpu_name = torch.cuda.get_device_name(gpu_idx)\n",
                "        total_mem = torch.cuda.get_device_properties(gpu_idx).total_memory / (1024**3)\n",
                "        print(f\"GPU: {gpu_name}\")\n",
                "        print(f\"Total VRAM: {total_mem:.2f} GB\")\n",
                "    else:\n",
                "        print(\"No CUDA-compatible GPU found.\")\n",
                "\n",
                "def get_config_text(n_clusters, overlay_ratio, stride, model_name, filename, version):\n",
                "    \"\"\"Generate a formatted string of configuration parameters.\"\"\"\n",
                "    config_lines = [\n",
                "        f\"Version: {version}\",\n",
                "        f\"Clusters: {n_clusters}\",\n",
                "        f\"Overlay Ratio: {overlay_ratio}\",\n",
                "        f\"Stride: {stride}\",\n",
                "        f\"Model: {model_name}\",\n",
                "        f\"File: {filename if filename else 'All files in directory'}\"\n",
                "    ]\n",
                "    return \"\\n\".join(config_lines)\n",
                "\n",
                "def initialize_model(stride, model_name, device):\n",
                "    model = HighResDV2(model_name, stride=stride, dtype=torch.float16).to(device)\n",
                "    model.eval()\n",
                "    shift_transforms, shift_inv_transforms = get_shift_transforms(dists=[1], pattern=\"Moore\")\n",
                "    flip_transforms, flip_inv_transforms = get_flip_transforms()\n",
                "    rot_transforms, rot_inv_transforms = get_rotation_transforms()\n",
                "    all_fwd_transforms, all_inv_transforms = combine_transforms(\n",
                "        shift_transforms, flip_transforms, shift_inv_transforms, flip_inv_transforms\n",
                "    )\n",
                "    all_transforms = [t for t in all_fwd_transforms if t != iden_partial]\n",
                "    all_inv_transforms = [t for t in all_inv_transforms if t != iden_partial]\n",
                "    model.set_transforms(all_transforms, all_inv_transforms)\n",
                "    return model\n",
                "\n",
                "def get_preprocess():\n",
                "    return Compose([\n",
                "        Resize((518, 518)),\n",
                "        ToTensor(),\n",
                "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "ba4a3aee-8019-4ee1-854d-956455cca4ca",
                "_uuid": "12655112-7ac1-47f4-9aa5-785f9a808768",
                "collapsed": false,
                "execution": {
                    "iopub.execute_input": "2025-06-11T14:15:04.751360Z",
                    "iopub.status.busy": "2025-06-11T14:15:04.751068Z",
                    "iopub.status.idle": "2025-06-11T14:15:04.767145Z",
                    "shell.execute_reply": "2025-06-11T14:15:04.766334Z",
                    "shell.execute_reply.started": "2025-06-11T14:15:04.751337Z"
                },
                "jupyter": {
                    "outputs_hidden": false
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Processing cell with automatic K selection (Elbow Method Only)\n",
                "\n",
                "def find_optimal_k_elbow(features_flat, k_range=(3, 10)):\n",
                "    \"\"\"\n",
                "    Find optimal K using enhanced elbow method optimized for tree segmentation.\n",
                "    \n",
                "    Args:\n",
                "        features_flat: Flattened feature array\n",
                "        k_range: Tuple of (min_k, max_k) - default (3,10) optimized for tree species\n",
                "    \n",
                "    Returns:\n",
                "        optimal_k: Best number of clusters\n",
                "        scores: Dictionary with analysis results\n",
                "    \"\"\"\n",
                "    min_k, max_k = k_range\n",
                "    k_values = list(range(min_k, max_k + 1))\n",
                "    wcss = []\n",
                "    \n",
                "    print(f\"🔍 Testing K values from {min_k} to {max_k} using elbow method...\")\n",
                "    \n",
                "    # Calculate WCSS for each K\n",
                "    for k in k_values:\n",
                "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
                "        kmeans.fit(features_flat)\n",
                "        wcss.append(kmeans.inertia_)\n",
                "        print(f\"   K={k}: WCSS={wcss[-1]:.2f}\")\n",
                "    \n",
                "    # Enhanced elbow detection with multiple methods\n",
                "    wcss_array = np.array(wcss)\n",
                "    \n",
                "    # Method 1: Second derivative (curvature)\n",
                "    if len(wcss_array) >= 3:\n",
                "        first_diff = np.diff(wcss_array)\n",
                "        second_diff = np.diff(first_diff)\n",
                "        curvature_idx = np.argmax(np.abs(second_diff)) + 1  # +1 due to diff operations\n",
                "    else:\n",
                "        curvature_idx = 0\n",
                "    \n",
                "    # Method 2: Percentage decrease threshold\n",
                "    pct_decrease = []\n",
                "    for i in range(1, len(wcss_array)):\n",
                "        pct = (wcss_array[i-1] - wcss_array[i]) / wcss_array[i-1] * 100\n",
                "        pct_decrease.append(pct)\n",
                "    \n",
                "    # Find where percentage decrease drops below threshold (diminishing returns)\n",
                "    threshold_idx = 0\n",
                "    for i, pct in enumerate(pct_decrease):\n",
                "        if pct < 5:  # Less than 5% improvement\n",
                "            threshold_idx = i\n",
                "            break\n",
                "    \n",
                "    # Choose the more conservative estimate (earlier elbow)\n",
                "    elbow_idx = min(int(curvature_idx), int(threshold_idx)) if threshold_idx > 0 else int(curvature_idx)\n",
                "    \n",
                "    # Safety bounds\n",
                "    elbow_idx = max(0, min(int(elbow_idx), len(k_values) - 1))\n",
                "    optimal_k = k_values[elbow_idx]\n",
                "    \n",
                "    # Validate result\n",
                "    if optimal_k < 3:\n",
                "        print(f\"⚠️  Optimal K={optimal_k} seems too low for tree species, using K=3\")\n",
                "        optimal_k = 3\n",
                "        elbow_idx = k_values.index(3) if 3 in k_values else 0\n",
                "    elif optimal_k > 8:\n",
                "        print(f\"⚠️  Optimal K={optimal_k} seems high for typical tree species, using K=8\")\n",
                "        optimal_k = min(8, max(k_values))\n",
                "        elbow_idx = k_values.index(optimal_k)\n",
                "    \n",
                "    print(f\"📊 Elbow method suggests optimal K = {optimal_k}\")\n",
                "    \n",
                "    return optimal_k, {\n",
                "        'k_values': k_values,\n",
                "        'wcss': wcss,\n",
                "        'elbow_idx': elbow_idx,\n",
                "        'optimal_k': optimal_k,\n",
                "        'pct_decrease': pct_decrease,\n",
                "        'method': 'elbow'\n",
                "    }\n",
                "\n",
                "def plot_elbow_analysis(scores, output_dir, output_prefix):\n",
                "    \"\"\"\n",
                "    Create enhanced elbow plot with additional analysis information.\n",
                "    \"\"\"\n",
                "    k_values = scores['k_values']\n",
                "    wcss = scores['wcss']\n",
                "    elbow_idx = scores['elbow_idx']\n",
                "    optimal_k = scores['optimal_k']\n",
                "    pct_decrease = scores.get('pct_decrease', [])\n",
                "    \n",
                "    # Create subplot with elbow curve and percentage decrease\n",
                "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
                "    \n",
                "    # Main elbow plot\n",
                "    ax1.plot(k_values, wcss, 'bo-', linewidth=3, markersize=10, alpha=0.7)\n",
                "    ax1.plot(k_values[elbow_idx], wcss[elbow_idx], 'ro', markersize=15, \n",
                "             label=f'Optimal K = {optimal_k}', zorder=5)\n",
                "    ax1.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
                "    ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
                "    ax1.set_title('🌳 Elbow Method for Tree Species Clustering', fontsize=14, fontweight='bold')\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    ax1.legend(fontsize=12)\n",
                "    \n",
                "    # Add annotations\n",
                "    ax1.annotate(f'Selected K = {optimal_k}', \n",
                "                xy=(k_values[elbow_idx], wcss[elbow_idx]),\n",
                "                xytext=(10, 10), textcoords='offset points',\n",
                "                bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
                "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
                "    \n",
                "    # Percentage decrease plot\n",
                "    if pct_decrease:\n",
                "        ax2.plot(k_values[1:], pct_decrease, 'go-', linewidth=2, markersize=8)\n",
                "        ax2.axhline(y=5, color='r', linestyle='--', alpha=0.7, label='5% Threshold')\n",
                "        ax2.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
                "        ax2.set_ylabel('WCSS Improvement (%)', fontsize=12)\n",
                "        ax2.set_title('Diminishing Returns Analysis', fontsize=12)\n",
                "        ax2.grid(True, alpha=0.3)\n",
                "        ax2.legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    \n",
                "    # Save plot\n",
                "    plot_path = os.path.join(output_dir, f\"{output_prefix}_elbow_analysis.png\")\n",
                "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
                "    plt.close()\n",
                "    \n",
                "    print(f\"📈 Saved elbow analysis: {plot_path}\")\n",
                "    return plot_path\n",
                "\n",
                "def process_image(image_path, model, preprocess, n_clusters, stride, version, device, \n",
                "                 auto_k=False, k_range=(3, 10)):\n",
                "    try:\n",
                "        print(f\"\\n--- Processing {image_path} ---\")\n",
                "        image = Image.open(image_path).convert(\"RGB\")\n",
                "        image_np = np.array(image)\n",
                "        h, w = image_np.shape[:2]\n",
                "        print(f\"Original image size: {w}x{h}\")\n",
                "\n",
                "        image_tensor = preprocess(image).to(device)\n",
                "        print(f\"Preprocessed tensor shape: {image_tensor.shape}\")\n",
                "\n",
                "        with torch.no_grad():\n",
                "            attn_choice = \"none\" if version == \"v1\" else \"o\"\n",
                "            features_out = model.forward_sequential(image_tensor, attn_choice=attn_choice)\n",
                "            print(f\"features_out type: {type(features_out)}\")\n",
                "            if isinstance(features_out, dict):\n",
                "                patch_features = features_out[\"x_norm_patchtokens\"]\n",
                "                attn_features = features_out.get(\"x_patchattn\", None) if version == \"v1.5\" else None\n",
                "                print(f\"patch_features shape: {patch_features.shape}\")\n",
                "                if attn_features is not None:\n",
                "                    print(f\"attn_features shape: {attn_features.shape}\")\n",
                "                patch_features = patch_features.mean(dim=0).squeeze()\n",
                "                if attn_features is not None:\n",
                "                    attn_features = attn_features.mean(dim=0).squeeze()\n",
                "            else:\n",
                "                print(f\"features_out shape: {getattr(features_out, 'shape', 'N/A')}\")\n",
                "                if hasattr(features_out, \"dim\") and features_out.dim() == 4:\n",
                "                    features = features_out.mean(dim=0)\n",
                "                else:\n",
                "                    features = features_out\n",
                "                H = W = 518 // stride\n",
                "                features = features.unsqueeze(0)\n",
                "                features = torch.nn.functional.interpolate(\n",
                "                    features, size=(H, W), mode=\"bilinear\", align_corners=False\n",
                "                ).squeeze(0)\n",
                "                features = features.permute(1, 2, 0)\n",
                "                patch_features = features\n",
                "                attn_features = None\n",
                "\n",
                "        if patch_features.dim() == 2:\n",
                "            n_patches = patch_features.shape[0]\n",
                "            H = W = int(np.sqrt(n_patches))\n",
                "            if H * W != n_patches:\n",
                "                raise ValueError(f\"Cannot infer H, W from n_patches={n_patches}\")\n",
                "            patch_features = patch_features.view(H, W, -1)\n",
                "            if attn_features is not None:\n",
                "                attn_features = attn_features.view(H, W, -1)\n",
                "        else:\n",
                "            H, W = patch_features.shape[:2]\n",
                "        print(f\"patch_features reshaped: {patch_features.shape}\")\n",
                "        if attn_features is not None:\n",
                "            print(f\"attn_features reshaped: {attn_features.shape}\")\n",
                "\n",
                "        if attn_features is not None and version == \"v1.5\":\n",
                "            features_np = np.concatenate(\n",
                "                [patch_features.cpu().numpy(), attn_features.cpu().numpy()], axis=-1\n",
                "            )\n",
                "            print(f\"Combined features shape: {features_np.shape}\")\n",
                "        else:\n",
                "            features_np = patch_features.cpu().numpy()\n",
                "            print(f\"Patch-only features shape: {features_np.shape}\")\n",
                "\n",
                "        features_flat = features_np.reshape(-1, features_np.shape[-1])\n",
                "        print(f\"Flattened features shape: {features_flat.shape}\")\n",
                "\n",
                "        if features_flat.shape[-1] > 128:\n",
                "            print(\"Running PCA on flat features...\")\n",
                "            features_flat_tensor = torch.tensor(features_flat, dtype=torch.float32)\n",
                "            mean = features_flat_tensor.mean(dim=0)\n",
                "            features_flat_centered = features_flat_tensor - mean\n",
                "            U, S, V = torch.pca_lowrank(features_flat_centered, q=128, center=False)\n",
                "            features_flat = (features_flat_centered @ V[:, :128]).numpy()\n",
                "            print(f\"PCA-reduced features shape: {features_flat.shape}\")\n",
                "\n",
                "        # Automatic K selection using elbow method\n",
                "        if auto_k:\n",
                "            print(f\"\\n--- Automatic K Selection using elbow method ---\")\n",
                "            optimal_k, k_scores = find_optimal_k_elbow(features_flat, k_range)\n",
                "            \n",
                "            print(f\"Selected optimal K = {optimal_k}\")\n",
                "            \n",
                "            # Save K selection analysis plot\n",
                "            output_dir = \"/kaggle/working/output\"\n",
                "            output_prefix = os.path.splitext(os.path.basename(image_path))[0]\n",
                "            plot_elbow_analysis(k_scores, output_dir, output_prefix)\n",
                "            \n",
                "            n_clusters = optimal_k\n",
                "        else:\n",
                "            print(f\"Using fixed K = {n_clusters}\")\n",
                "\n",
                "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
                "        labels = kmeans.fit_predict(features_flat)\n",
                "        print(f\"labels shape after kmeans: {labels.shape}\")\n",
                "        labels = labels.reshape(H, W)\n",
                "        print(f\"Labels shape after reshape: {labels.shape}\")\n",
                "\n",
                "        labels_resized = cv2.resize(\n",
                "            labels.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST\n",
                "        )\n",
                "        print(f\"labels_resized shape: {labels_resized.shape}\")\n",
                "\n",
                "        return image_np, labels_resized\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {image_path}: {e}\")\n",
                "        traceback.print_exc()\n",
                "        return None, None\n",
                "\n",
                "def run_processing(\n",
                "    input_dir=\"input\",\n",
                "    output_dir=\"output\",\n",
                "    n_clusters=5,\n",
                "    overlay_ratio=5,\n",
                "    stride=4,\n",
                "    model_name=\"dinov2_vits14\",\n",
                "    filename=None,\n",
                "    version=\"v1.5\"\n",
                "):\n",
                "    print_gpu_info()\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    overlay_ratio = float(overlay_ratio)\n",
                "    if overlay_ratio < 1 or overlay_ratio > 10:\n",
                "        print(\"overlay_ratio must be between 1 and 10. Using default value 5.\")\n",
                "        overlay_ratio = 5\n",
                "\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    print(f\"Using device: {device}\")\n",
                "\n",
                "    model = initialize_model(stride, model_name, device)\n",
                "    preprocess = get_preprocess()\n",
                "\n",
                "    if filename:\n",
                "        image_path = os.path.join(input_dir, filename)\n",
                "        if os.path.exists(image_path) and filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")):\n",
                "            output_prefix = os.path.splitext(filename)[0]\n",
                "            print(f\"Processing {filename} ...\")\n",
                "            return process_image(image_path, model, preprocess, n_clusters, stride, version, device, \n",
                "                               auto_k=False, k_range=(3, 10)), output_prefix\n",
                "        else:\n",
                "            print(f\"File {filename} not found or is not a supported image format.\")\n",
                "            return None, None\n",
                "    else:\n",
                "        results = []\n",
                "        for filename in os.listdir(input_dir):\n",
                "            if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")):\n",
                "                image_path = os.path.join(input_dir, filename)\n",
                "                output_prefix = os.path.splitext(filename)[0]\n",
                "                print(f\"Processing {filename} ...\")\n",
                "                result = process_image(image_path, model, preprocess, n_clusters, stride, version, device,\n",
                "                                      auto_k=False, k_range=(3, 10))\n",
                "                results.append((result, output_prefix))\n",
                "        return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Updated Entry point cell with automatic K selection\n",
                "import sys\n",
                "sys.path.append(\"/kaggle/working/project/src\")\n",
                "\n",
                "# Configuration with automatic K selection options\n",
                "config = {\n",
                "    \"input_dir\": \"/kaggle/input/drone-10-best\",\n",
                "    \"output_dir\": \"/kaggle/working/output\",\n",
                "    \"n_clusters\": 6,  # This will be ignored if auto_k=True\n",
                "    \"overlay_ratio\": 4,\n",
                "    \"stride\": 4,\n",
                "    \"model_name\": \"dinov2_vits14\",\n",
                "    \"filename\": \"DJI_20250127150117_0029_D.JPG\",\n",
                "    \"version\": \"v1.5\",\n",
                "    \n",
                "    # NEW: Automatic K selection parameters  \n",
                "    \"auto_k\": True,  # Set to True to enable automatic K selection\n",
                "    \"k_range\": (3, 10),  # Range of K values to test (optimized for tree species)\n",
                "}\n",
                "\n",
                "def tree_seg_with_auto_k(\n",
                "    input_dir=\"input\",\n",
                "    output_dir=\"output\",\n",
                "    n_clusters=5,\n",
                "    overlay_ratio=5,\n",
                "    stride=2,\n",
                "    model_name=\"dinov2_vits14\",\n",
                "    filename=None,\n",
                "    version=\"v1.5\",\n",
                "    auto_k=False,\n",
                "    k_range=(3, 10)\n",
                "):\n",
                "    \"\"\"Enhanced tree segmentation with automatic K selection.\"\"\"\n",
                "    print_gpu_info()\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    overlay_ratio = float(overlay_ratio)\n",
                "    if overlay_ratio < 1 or overlay_ratio > 10:\n",
                "        print(\"overlay_ratio must be between 1 and 10. Using default value 5.\")\n",
                "        overlay_ratio = 5\n",
                "\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    print(f\"Using device: {device}\")\n",
                "\n",
                "    model = initialize_model(stride, model_name, device)\n",
                "    preprocess = get_preprocess()\n",
                "\n",
                "    if filename:\n",
                "        image_path = os.path.join(input_dir, filename)\n",
                "        if os.path.exists(image_path) and filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")):\n",
                "            output_prefix = os.path.splitext(filename)[0]\n",
                "            print(f\"Processing {filename} ...\")\n",
                "            \n",
                "            # Process with automatic K selection parameters\n",
                "            result = process_image(\n",
                "                image_path, model, preprocess, n_clusters, stride, version, device,\n",
                "                auto_k=auto_k, k_range=k_range, k_method=k_method\n",
                "            )\n",
                "            \n",
                "            if result[0] is not None:\n",
                "                image_np, labels_resized = result\n",
                "                # Get the actual number of clusters used (may be different if auto_k=True)\n",
                "                actual_n_clusters = len(np.unique(labels_resized))\n",
                "                \n",
                "                generate_outputs(\n",
                "                    image_np, labels_resized, output_prefix, output_dir,\n",
                "                    actual_n_clusters, overlay_ratio, stride, model_name,\n",
                "                    image_path, version\n",
                "                )\n",
                "                \n",
                "                print(f\"✅ Processing completed! Used K = {actual_n_clusters}\")\n",
                "                if auto_k:\n",
                "                    print(f\"📊 K selection method: {k_method}\")\n",
                "                    print(f\"📈 K selection analysis saved as: {output_prefix}_{k_method}_analysis.png\")\n",
                "            else:\n",
                "                print(\"❌ Processing failed\")\n",
                "        else:\n",
                "            print(f\"File {filename} not found or is not a supported image format.\")\n",
                "    else:\n",
                "        print(\"Processing all images in directory...\")\n",
                "        # Process all images with auto K selection\n",
                "        for fname in os.listdir(input_dir):\n",
                "            if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")):\n",
                "                image_path = os.path.join(input_dir, fname)\n",
                "                output_prefix = os.path.splitext(fname)[0]\n",
                "                print(f\"\\nProcessing {fname} ...\")\n",
                "                \n",
                "                result = process_image(\n",
                "                    image_path, model, preprocess, n_clusters, stride, version, device,\n",
                "                    auto_k=auto_k, k_range=k_range, k_method=k_method\n",
                "                )\n",
                "                \n",
                "                if result[0] is not None:\n",
                "                    image_np, labels_resized = result\n",
                "                    actual_n_clusters = len(np.unique(labels_resized))\n",
                "                    \n",
                "                    generate_outputs(\n",
                "                        image_np, labels_resized, output_prefix, output_dir,\n",
                "                        actual_n_clusters, overlay_ratio, stride, model_name,\n",
                "                        image_path, version\n",
                "                    )\n",
                "                    \n",
                "                    print(f\"✅ {fname} completed! Used K = {actual_n_clusters}\")\n",
                "\n",
                "# Run the enhanced segmentation\n",
                "print(\"🌳 Starting Enhanced Tree Segmentation with Automatic K Selection...\")\n",
                "print(f\"📁 Input: {config['input_dir']}\")\n",
                "print(f\"📁 Output: {config['output_dir']}\")\n",
                "print(f\"🔧 Auto K: {config['auto_k']}\")\n",
                "if config['auto_k']:\n",
                "    print(f\"📊 K Method: {config['k_method']}\")\n",
                "    print(f\"📈 K Range: {config['k_range']}\")\n",
                "else:\n",
                "    print(f\"🔢 Fixed K: {config['n_clusters']}\")\n",
                "\n",
                "tree_seg_with_auto_k(**config)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean Entry Point with Elbow-Only Auto K Selection\n",
                "import sys\n",
                "sys.path.append(\"/kaggle/working/project/src\")\n",
                "\n",
                "# Simplified configuration - Elbow Method Only\n",
                "config = {\n",
                "    \"input_dir\": \"/kaggle/input/drone-10-best\",\n",
                "    \"output_dir\": \"/kaggle/working/output\",\n",
                "    \"n_clusters\": 6,  # Used only if auto_k=False\n",
                "    \"overlay_ratio\": 4,\n",
                "    \"stride\": 4,\n",
                "    \"model_name\": \"dinov2_vits14\",\n",
                "    \"filename\": \"DJI_20250127150117_0029_D.JPG\",\n",
                "    \"version\": \"v1.5\",\n",
                "    \n",
                "    # Automatic K selection (Elbow Method)\n",
                "    \"auto_k\": True,           # Enable automatic K selection\n",
                "    \"k_range\": (3, 10),       # Optimal range for tree species\n",
                "}\n",
                "\n",
                "def tree_seg_elbow(\n",
                "    input_dir=\"input\",\n",
                "    output_dir=\"output\", \n",
                "    n_clusters=5,\n",
                "    overlay_ratio=5,\n",
                "    stride=4,\n",
                "    model_name=\"dinov2_vits14\",\n",
                "    filename=None,\n",
                "    version=\"v1.5\",\n",
                "    auto_k=False,\n",
                "    k_range=(3, 10)\n",
                "):\n",
                "    \"\"\"Tree segmentation with automatic K selection using elbow method.\"\"\"\n",
                "    print_gpu_info()\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    \n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    print(f\"Using device: {device}\")\n",
                "\n",
                "    model = initialize_model(stride, model_name, device)\n",
                "    preprocess = get_preprocess()\n",
                "\n",
                "    if filename:\n",
                "        image_path = os.path.join(input_dir, filename)\n",
                "        if os.path.exists(image_path) and filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")):\n",
                "            output_prefix = os.path.splitext(filename)[0]\n",
                "            print(f\"Processing {filename} ...\")\n",
                "            \n",
                "            # Process with automatic K selection\n",
                "            result = process_image(\n",
                "                image_path, model, preprocess, n_clusters, stride, version, device,\n",
                "                auto_k=auto_k, k_range=k_range\n",
                "            )\n",
                "            \n",
                "            if result[0] is not None:\n",
                "                image_np, labels_resized = result\n",
                "                actual_n_clusters = len(np.unique(labels_resized))\n",
                "                \n",
                "                generate_outputs(\n",
                "                    image_np, labels_resized, output_prefix, output_dir,\n",
                "                    actual_n_clusters, overlay_ratio, stride, model_name,\n",
                "                    image_path, version\n",
                "                )\n",
                "                \n",
                "                print(f\"✅ Processing completed! Used K = {actual_n_clusters}\")\n",
                "                if auto_k:\n",
                "                    print(f\"📊 Elbow method analysis saved\")\n",
                "            else:\n",
                "                print(\"❌ Processing failed\")\n",
                "        else:\n",
                "            print(f\"File {filename} not found or is not a supported image format.\")\n",
                "\n",
                "# Run the segmentation\n",
                "print(\"🌳 Enhanced Tree Segmentation with Automatic K Selection (Elbow Method)\")\n",
                "print(f\"📁 Input: {config['input_dir']}\")\n",
                "print(f\"📁 Output: {config['output_dir']}\")\n",
                "print(f\"🔧 Auto K: {config['auto_k']}\")\n",
                "if config['auto_k']:\n",
                "    print(f\"📈 K Range: {config['k_range']} (optimized for tree species)\")\n",
                "else:\n",
                "    print(f\"🔢 Fixed K: {config['n_clusters']}\")\n",
                "\n",
                "tree_seg_elbow(**config)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "e12212cd-629d-4b46-9023-0b2824cede15",
                "_uuid": "e3c2431d-2435-4813-b08a-ec4787fe4b60",
                "execution": {
                    "iopub.execute_input": "2025-06-11T14:15:08.701873Z",
                    "iopub.status.busy": "2025-06-11T14:15:08.701220Z",
                    "iopub.status.idle": "2025-06-11T14:15:08.714954Z",
                    "shell.execute_reply": "2025-06-11T14:15:08.714296Z",
                    "shell.execute_reply.started": "2025-06-11T14:15:08.701849Z"
                },
                "jupyter": {
                    "source_hidden": true
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Visualization cell\n",
                "def generate_outputs(\n",
                "    image_np,\n",
                "    labels_resized,\n",
                "    output_prefix,\n",
                "    output_dir,\n",
                "    n_clusters,\n",
                "    overlay_ratio,\n",
                "    stride,\n",
                "    model_name,\n",
                "    image_path,\n",
                "    version,\n",
                "):\n",
                "    if image_np is None or labels_resized is None:\n",
                "        print(f\"Skipping output generation for {image_path} due to processing error.\")\n",
                "        return\n",
                "\n",
                "    alpha = (10 - overlay_ratio) / 10.0\n",
                "    filename = os.path.basename(image_path)\n",
                "\n",
                "    if n_clusters <= 10:\n",
                "        cmap = plt.get_cmap(\"tab10\", n_clusters)\n",
                "    elif n_clusters <= 20:\n",
                "        cmap = plt.get_cmap(\"tab20\", n_clusters)\n",
                "    else:\n",
                "        cmap = plt.get_cmap(\"gist_ncar\", n_clusters)\n",
                "\n",
                "    config_text = get_config_text(n_clusters, overlay_ratio, stride, model_name, filename, version)\n",
                "    fig, ax = plt.subplots(figsize=(10, 10))\n",
                "    im = ax.imshow(labels_resized, cmap=cmap, vmin=0, vmax=n_clusters - 1)\n",
                "    cbar = plt.colorbar(im, ax=ax, ticks=range(n_clusters))\n",
                "    cbar.ax.set_yticklabels([f\"Cluster {i}\" for i in range(n_clusters)])\n",
                "    ax.axis(\"off\")\n",
                "    ax.text(\n",
                "        0.02, 0.98, config_text,\n",
                "        transform=ax.transAxes, fontsize=8,\n",
                "        verticalalignment='top', horizontalalignment='left',\n",
                "        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')\n",
                "    )\n",
                "    plt.tight_layout()\n",
                "    legend_path = os.path.join(output_dir, f\"{output_prefix}_segmentation_legend.png\")\n",
                "    plt.savefig(legend_path, bbox_inches=\"tight\", pad_inches=0.1, dpi=200)\n",
                "    plt.close()\n",
                "    print(f\"Saved segmentation with legend: {legend_path}\")\n",
                "\n",
                "    norm = colors.Normalize(vmin=0, vmax=n_clusters - 1)\n",
                "    segmentation_rgb = cmap(norm(labels_resized))[:, :, :3]\n",
                "    segmentation_rgb = (segmentation_rgb * 255).astype(np.uint8)\n",
                "    overlay = (alpha * image_np + (1 - alpha) * segmentation_rgb).astype(np.uint8)\n",
                "    fig, ax = plt.subplots(figsize=(10, 10))\n",
                "    ax.imshow(overlay)\n",
                "    ax.axis(\"off\")\n",
                "    ax.text(\n",
                "        0.02, 0.98, config_text,\n",
                "        transform=ax.transAxes, fontsize=8,\n",
                "        verticalalignment='top', horizontalalignment='left',\n",
                "        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')\n",
                "    )\n",
                "    plt.tight_layout()\n",
                "    overlay_path = os.path.join(output_dir, f\"{output_prefix}_overlay.png\")\n",
                "    plt.savefig(overlay_path, bbox_inches=\"tight\", pad_inches=0.1, dpi=200)\n",
                "    plt.close()\n",
                "    print(f\"Saved overlay: {overlay_path}\")\n",
                "\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
                "    axes[0].imshow(image_np)\n",
                "    axes[0].set_title(\"Original Image\", fontsize=12)\n",
                "    axes[0].axis(\"off\")\n",
                "    im = axes[1].imshow(labels_resized, cmap=cmap, vmin=0, vmax=n_clusters - 1)\n",
                "    axes[1].set_title(\"Segmentation Map\", fontsize=12)\n",
                "    axes[1].axis(\"off\")\n",
                "    cbar = fig.colorbar(im, ax=axes[1], ticks=range(n_clusters))\n",
                "    cbar.ax.set_yticklabels([f\"Cluster {i}\" for i in range(n_clusters)])\n",
                "    axes[1].text(\n",
                "        0.02, 0.98, config_text,\n",
                "        transform=axes[1].transAxes, fontsize=8,\n",
                "        verticalalignment='top', horizontalalignment='left',\n",
                "        bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')\n",
                "    )\n",
                "    plt.tight_layout()\n",
                "    side_by_side_path = os.path.join(output_dir, f\"{output_prefix}_side_by_side.png\")\n",
                "    plt.savefig(side_by_side_path, bbox_inches=\"tight\", pad_inches=0.1, dpi=200)\n",
                "    plt.close()\n",
                "    print(f\"Saved side-by-side image: {side_by_side_path}\")\n",
                "\n",
                "def run_visualization(\n",
                "    input_dir=\"input\",\n",
                "    output_dir=\"output\",\n",
                "    n_clusters=5,\n",
                "    overlay_ratio=5,\n",
                "    stride=4,\n",
                "    model_name=\"dinov2_vits14\",\n",
                "    filename=None,\n",
                "    version=\"v1.5\"\n",
                "):\n",
                "    results = run_processing(\n",
                "        input_dir, output_dir, n_clusters, overlay_ratio, stride, model_name, filename, version\n",
                "    )\n",
                "    if filename:\n",
                "        if results[0][0] is not None:\n",
                "            (image_np, labels_resized), output_prefix = results\n",
                "            generate_outputs(\n",
                "                image_np, labels_resized, output_prefix, output_dir,\n",
                "                n_clusters, overlay_ratio, stride, model_name,\n",
                "                os.path.join(input_dir, filename), version\n",
                "            )\n",
                "    else:\n",
                "        for (image_np, labels_resized), output_prefix in results:\n",
                "            if image_np is not None:\n",
                "                generate_outputs(\n",
                "                    image_np, labels_resized, output_prefix, output_dir,\n",
                "                    n_clusters, overlay_ratio, stride, model_name,\n",
                "                    os.path.join(input_dir, output_prefix + \".jpg\"), version\n",
                "                )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "db16b935-a38d-499d-957d-802d1ff43125",
                "_uuid": "0a185bac-c287-4770-8f47-49aeb06f78f2",
                "collapsed": false,
                "execution": {
                    "iopub.execute_input": "2025-06-11T14:15:11.773452Z",
                    "iopub.status.busy": "2025-06-11T14:15:11.772946Z",
                    "iopub.status.idle": "2025-06-11T14:15:49.860433Z",
                    "shell.execute_reply": "2025-06-11T14:15:49.859748Z",
                    "shell.execute_reply.started": "2025-06-11T14:15:11.773428Z"
                },
                "jupyter": {
                    "outputs_hidden": false
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Entry point cell\n",
                "import sys\n",
                "sys.path.append(\"/kaggle/working/project/src\")\n",
                "\n",
                "config = {\n",
                "    \"input_dir\": \"/kaggle/input/drone-10-best\",\n",
                "    \"output_dir\": \"/kaggle/working/output\",\n",
                "    \"n_clusters\": 6,\n",
                "    \"overlay_ratio\": 4,\n",
                "    \"stride\": 4,\n",
                "    \"model_name\": \"dinov2_vits14\",\n",
                "    \"filename\": \"DJI_20250127150117_0029_D.JPG\",\n",
                "    \"version\": \"v1.5\"  # Options: \"v1\" (patch features only) or \"v1.5\" (patch + attention features)\n",
                "}\n",
                "\n",
                "def tree_seg(\n",
                "    input_dir=\"input\",\n",
                "    output_dir=\"output\",\n",
                "    n_clusters=5,\n",
                "    overlay_ratio=5,\n",
                "    stride=2, # using 2 instead of 4 to increase resolution\n",
                "    model_name=\"dinov2_vits14\",\n",
                "    filename=None,\n",
                "    version=\"v1.5\"\n",
                "):\n",
                "    run_visualization(input_dir, output_dir, n_clusters, overlay_ratio, stride, model_name, filename, version)\n",
                "\n",
                "# Run the segmentation\n",
                "tree_seg(**config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "1a721004-ed2b-43b9-8250-95b00b164540",
                "_uuid": "9bdc2002-f76a-4936-9935-52988dfdc312",
                "collapsed": false,
                "execution": {
                    "iopub.execute_input": "2025-06-11T13:55:31.996128Z",
                    "iopub.status.busy": "2025-06-11T13:55:31.995804Z",
                    "iopub.status.idle": "2025-06-11T13:55:32.120510Z",
                    "shell.execute_reply": "2025-06-11T13:55:32.119737Z",
                    "shell.execute_reply.started": "2025-06-11T13:55:31.996097Z"
                },
                "jupyter": {
                    "outputs_hidden": false
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "from IPython.display import Image, display\n",
                "\n",
                "# Paths to the output files\n",
                "filename = config[\"filename\"]\n",
                "output_prefix = os.path.splitext(filename)[0]\n",
                "legend_path = os.path.join(config[\"output_dir\"], f\"{output_prefix}_segmentation_legend.png\")\n",
                "overlay_path = os.path.join(config[\"output_dir\"], f\"{output_prefix}_overlay.png\")\n",
                "side_by_side_path = os.path.join(config[\"output_dir\"], f\"{output_prefix}_side_by_side.png\")\n",
                "\n",
                "# # Display the segmentation map with legend\n",
                "# if os.path.exists(legend_path):\n",
                "#     print(\"Segmentation Map with Legend:\")\n",
                "#     display(Image(filename=legend_path))\n",
                "\n",
                "# Display the overlay\n",
                "if os.path.exists(overlay_path):\n",
                "    print(\"Overlay Image:\")\n",
                "    display(Image(filename=overlay_path))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "98fd6d47-ebe6-4aff-8821-6dc68d76ac24",
                "_uuid": "06120845-5752-48c1-86a7-e36abaf01ab9",
                "collapsed": false,
                "execution": {
                    "iopub.execute_input": "2025-06-11T13:55:32.121592Z",
                    "iopub.status.busy": "2025-06-11T13:55:32.121365Z",
                    "iopub.status.idle": "2025-06-11T13:55:32.280727Z",
                    "shell.execute_reply": "2025-06-11T13:55:32.279907Z",
                    "shell.execute_reply.started": "2025-06-11T13:55:32.121576Z"
                },
                "jupyter": {
                    "outputs_hidden": false
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Display the side-by-side image\n",
                "if os.path.exists(side_by_side_path):\n",
                "    print(\"Original and Segmentation Side by Side:\")\n",
                "    display(Image(filename=side_by_side_path))"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [
                {
                    "datasetId": 7585067,
                    "sourceId": 12052366,
                    "sourceType": "datasetVersion"
                }
            ],
            "dockerImageVersionId": 31041,
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
